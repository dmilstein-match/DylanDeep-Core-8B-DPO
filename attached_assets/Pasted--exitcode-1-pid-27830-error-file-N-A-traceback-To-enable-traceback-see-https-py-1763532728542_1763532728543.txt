  exitcode  : 1 (pid: 27830)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$ sed -i 's/model\.gradient_checkpointing_enable()/# model.gradient_checkpointing_enable()/' src/baseline_sft/train_sft_abel.py
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$ sed -i 's/gradient_checkpointing=True/gradient_checkpointing=False/' src/baseline_sft/train_sft_abel.py
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$ torchrun --nproc_per_node=8 src/baseline_sft/train_sft_abel.py
W1119 06:04:13.170000 28487 torch/distributed/run.py:803]
W1119 06:04:13.170000 28487 torch/distributed/run.py:803] *****************************************
W1119 06:04:13.170000 28487 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W1119 06:04:13.170000 28487 torch/distributed/run.py:803] *****************************************
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
================================================================================================================================================================================================================================================


Abel-7B-002 SFT Training with LoRAAbel-7B-002 SFT Training with LoRAAbel-7B-002 SFT Training with LoRA


================================================================================================================================================================================================================================================



Loading GSM8K training data from data/gsm8k_train.jsonl...
Loading GSM8K training data from data/gsm8k_train.jsonl...
Loading GSM8K training data from data/gsm8k_train.jsonl...


================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]Loading Abel base model in bf16...
Loading Abel base model in bf16...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.20s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.23s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.20s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.26s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.31s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.56s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.58s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.57s/it]

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.
  0%|                                                                                           | 0/234 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank0]:     main()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank0]:     return super().training_step(*args, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank0]:     return user_fn(self, *args)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank0]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank0]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank7]:     main()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank7]:     trainer.train()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank7]:     return super().training_step(*args, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank7]:     loss.backward(**kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank7]:     return user_fn(self, *args)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank7]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank7]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank1]:     main()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank1]:     trainer.train()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank1]:     return super().training_step(*args, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank1]:     return user_fn(self, *args)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank1]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank1]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank5]:     main()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank5]:     trainer.train()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank5]:     return super().training_step(*args, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank5]:     loss.backward(**kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank5]:     return user_fn(self, *args)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank5]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank5]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank4]:     main()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank4]:     trainer.train()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank4]:     return super().training_step(*args, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank4]:     loss.backward(**kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank4]:     return user_fn(self, *args)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank4]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank4]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank3]:     main()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank3]:     trainer.train()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank3]:     return super().training_step(*args, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank3]:     loss.backward(**kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank3]:     return user_fn(self, *args)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank3]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank3]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank6]:     main()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank6]:     trainer.train()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank6]:     return super().training_step(*args, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank6]:     loss.backward(**kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank6]:     return user_fn(self, *args)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank6]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank6]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 149, in <module>
[rank2]:     main()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 135, in main
[rank2]:     trainer.train()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank2]:     return super().training_step(*args, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank2]:     return user_fn(self, *args)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank2]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank2]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
  0%|                                                                                           | 0/234 [00:00<?, ?it/s]
[rank0]:[W1119 06:04:40.598247798 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1119 06:04:42.761000 28487 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 28581 closing signal SIGTERM
W1119 06:04:42.761000 28487 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 28582 closing signal SIGTERM
W1119 06:04:42.761000 28487 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 28583 closing signal SIGTERM
W1119 06:04:42.762000 28487 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 28584 closing signal SIGTERM
W1119 06:04:42.762000 28487 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 28586 closing signal SIGTERM
W1119 06:04:42.762000 28487 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 28587 closing signal SIGTERM
W1119 06:04:42.762000 28487 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 28588 closing signal SIGTERM
E1119 06:04:43.654000 28487 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 4 (pid: 28585) of binary: /home/ubuntu/looper-math-platinum-8b/.venv/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/looper-math-platinum-8b/.venv/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
src/baseline_sft/train_sft_abel.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-19_06:04:42
  host      : 192-222-54-90
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 28585)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$