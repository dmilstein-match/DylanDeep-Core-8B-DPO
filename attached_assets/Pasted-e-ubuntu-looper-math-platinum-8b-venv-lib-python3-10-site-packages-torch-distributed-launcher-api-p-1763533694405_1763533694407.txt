e/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
src/baseline_sft/train_sft_abel.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-11-19_06:21:43
  host      : 192-222-54-90
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 30951)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-11-19_06:21:43
  host      : 192-222-54-90
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 30953)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-11-19_06:21:43
  host      : 192-222-54-90
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 30954)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-11-19_06:21:43
  host      : 192-222-54-90
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 30956)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-19_06:21:43
  host      : 192-222-54-90
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 30950)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$ TORCH_DISTRIBUTED_DEBUG=DETAIL accelerate launch --config_file .accelerate_config.yaml src/baseline_sft/train_sft_abel.py
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]Loading Abel base model in bf16...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
Loading Abel base model in bf16...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.11s/it]
Disabling gradient checkpointing for DDP compatibility...
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.19s/it]
Disabling gradient checkpointing for DDP compatibility...
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.21s/it]
Disabling gradient checkpointing for DDP compatibility...
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.35s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.38s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.33s/it]
Disabling gradient checkpointing for DDP compatibility...
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.37s/it]
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.45s/it]
Disabling gradient checkpointing for DDP compatibility...
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7


Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.
  0%|                                                                                           | 0/234 [00:00<?, ?it/s][rank4]: Traceback (most recent call last):
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank4]:     main()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank4]:     trainer.train()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank4]:     return super().training_step(*args, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank4]:     loss.backward(**kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank4]:     return user_fn(self, *args)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank4]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank4]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank6]:     main()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank6]:     trainer.train()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank6]:     return super().training_step(*args, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank6]:     loss.backward(**kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank6]:     return user_fn(self, *args)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank6]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank6]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank7]:     main()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank7]:     trainer.train()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank7]:     return super().training_step(*args, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank7]:     loss.backward(**kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank7]:     return user_fn(self, *args)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank7]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank7]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank2]:     main()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank2]:     trainer.train()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank2]:     return super().training_step(*args, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank2]:     return user_fn(self, *args)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank2]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank2]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank3]:     main()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank3]:     trainer.train()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank3]:     return super().training_step(*args, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank3]:     loss.backward(**kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank3]:     return user_fn(self, *args)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank3]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank3]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank0]:     main()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank0]:     return super().training_step(*args, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank0]:     return user_fn(self, *args)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank0]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank0]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank5]:     main()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank5]:     trainer.train()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank5]:     return super().training_step(*args, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank5]:     loss.backward(**kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank5]:     return user_fn(self, *args)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank5]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank5]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank1]:     main()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank1]:     trainer.train()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank1]:     return super().training_step(*args, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank1]:     return user_fn(self, *args)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank1]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank1]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
  0%|                                                                                           | 0/234 [00:01<?, ?it/s]
[rank0]:[W1119 06:23:11.358062816 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1119 06:23:13.511000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31440 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31442 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31443 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31444 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31445 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31446 closing signal SIGTERM
W1119 06:23:13.513000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31447 closing signal SIGTERM
E1119 06:23:14.128000 31340 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 31441) of binary: /home/ubuntu/looper-math-platinum-8b/.venv/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/looper-math-platinum-8b/.venv/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
src/baseline_sft/train_sft_abel.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-19_06:23:13
  host      : 192-222-54-90
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 31441)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$ git pull
remote: Enumerating objects: 14, done.
remote: Counting objects: 100% (14/14), done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 9 (delta 6), reused 9 (delta 6), pack-reused 0 (from 0)
Unpacking objects: 100% (9/9), 4.63 KiB | 4.63 MiB/s, done.
From github.com:dmilstein-match/looper-math-platinum-8b
   fd196ec..7331c91  main       -> origin/main
Updating fd196ec..7331c91
Fast-forward
 ...-the-model-config-and-generation-config--1763533431054_1763533431055.txt | 301 ++++++++++++++++++++++++++++++++++++
 ...asted--venv-ubuntu-192-222-54-90-looper--1763533394042_1763533394042.txt | 114 ++++++++++++++
 src/baseline_sft/train_sft_abel.py                                          |   4 +-
 3 files changed, 417 insertions(+), 2 deletions(-)
 create mode 100644 attached_assets/Pasted--The-tokenizer-has-new-PAD-BOS-EOS-tokens-that-differ-from-the-model-config-and-generation-config--1763533431054_1763533431055.txt
 create mode 100644 attached_assets/Pasted--venv-ubuntu-192-222-54-90-looper--1763533394042_1763533394042.txt
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$ accelerate launch --config_file .accelerate_config.yaml src/baseline_sft/train_sft_abel.py
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL

⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
⚠️  GRADIENT CHECKPOINTING FORCEFULLY DISABLED AT PYTORCH LEVEL
================================================================================
Abel-7B-002 SFT Training with LoRA
================================================================================

Loading GSM8K training data from data/gsm8k_train.jsonl...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loaded 7473 training examples

Loading tokenizer from GAIR/Abel-7B-002...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]Loading Abel base model in bf16...
Loading Abel base model in bf16...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading Abel base model in bf16...
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.14s/it]
Disabling gradient checkpointing for DDP compatibility...
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.39s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.37s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.40s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.38s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.37s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.40s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.37s/it]
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...
Disabling gradient checkpointing for DDP compatibility...

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.
  0%|                                                                                           | 0/234 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank0]:     main()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank0]:     return super().training_step(*args, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank0]:     return user_fn(self, *args)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank0]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank0]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank2]:     main()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank2]:     trainer.train()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank2]:     return super().training_step(*args, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank2]:     return user_fn(self, *args)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank2]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank2]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank6]:     main()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank6]:     trainer.train()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank6]:     return super().training_step(*args, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank6]:     loss.backward(**kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank6]:     return user_fn(self, *args)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank6]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank6]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank1]:     main()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank1]:     trainer.train()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank1]:     return super().training_step(*args, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank1]:     return user_fn(self, *args)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank1]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank1]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank7]:     main()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank7]:     trainer.train()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank7]:     return super().training_step(*args, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank7]:     loss.backward(**kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank7]:     return user_fn(self, *args)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank7]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank7]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank5]:     main()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank5]:     trainer.train()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank5]:     return super().training_step(*args, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank5]:     loss.backward(**kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank5]:     return user_fn(self, *args)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank5]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank5]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank3]:     main()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank3]:     trainer.train()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank3]:     return super().training_step(*args, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank3]:     loss.backward(**kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank3]:     return user_fn(self, *args)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank3]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank3]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank4]:     main()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank4]:     trainer.train()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank4]:     return super().training_step(*args, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank4]:     loss.backward(**kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank4]:     return user_fn(self, *args)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank4]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank4]: Parameter at index 255 with name base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
  0%|                                                                                           | 0/234 [00:00<?, ?it/s]
[rank0]:[W1119 06:27:46.205499882 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1119 06:27:48.314000 32164 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 32263 closing signal SIGTERM
W1119 06:27:48.315000 32164 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 32264 closing signal SIGTERM
W1119 06:27:48.315000 32164 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 32265 closing signal SIGTERM
W1119 06:27:48.315000 32164 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 32266 closing signal SIGTERM
W1119 06:27:48.315000 32164 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 32267 closing signal SIGTERM
W1119 06:27:48.315000 32164 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 32269 closing signal SIGTERM
W1119 06:27:48.315000 32164 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 32270 closing signal SIGTERM
E1119 06:27:49.108000 32164 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 5 (pid: 32268) of binary: /home/ubuntu/looper-math-platinum-8b/.venv/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/looper-math-platinum-8b/.venv/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
src/baseline_sft/train_sft_abel.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-19_06:27:48
  host      : 192-222-54-90
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 32268)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$