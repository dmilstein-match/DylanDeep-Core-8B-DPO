2-11.7.3.90
  Attempting uninstall: nvidia-cudnn-cu12
    Found existing installation: nvidia-cudnn-cu12 9.10.2.21
    Uninstalling nvidia-cudnn-cu12-9.10.2.21:
      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21
  Attempting uninstall: torch
    Found existing installation: torch 2.9.1
    Uninstalling torch-2.9.1:
      Successfully uninstalled torch-2.9.1
Successfully installed annotated-doc-0.0.4 annotated-types-0.7.0 click-8.3.1 cloudpickle-3.1.2 compressed-tensors-0.6.0 diskcache-5.6.3 distro-1.9.0 einops-0.8.1 fastapi-0.121.2 gguf-0.10.0 httptools-0.7.1 importlib-metadata-8.7.0 interegular-0.3.3 jiter-0.12.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 lark-1.3.1 llvmlite-0.45.1 lm-format-enforcer-0.10.6 mistral-common-1.8.5 msgpack-1.1.2 msgspec-0.19.0 nest-asyncio-1.6.0 numba-0.62.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-13.580.82 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 openai-2.8.1 opencv-python-headless-4.11.0.86 outlines-0.0.46 partial-json-parser-0.2.1.1.post7 pillow-12.0.0 prometheus-client-0.23.1 prometheus-fastapi-instrumentator-7.1.0 protobuf-6.33.1 py-cpuinfo-9.0.0 pyairports-0.0.1 pycountry-24.6.1 pydantic-2.12.4 pydantic-core-2.41.5 pydantic-extra-types-2.10.6 python-dotenv-1.2.1 pyzmq-27.1.0 ray-2.51.1 referencing-0.37.0 rpds-py-0.29.0 starlette-0.49.3 tiktoken-0.12.0 torch-2.4.0 torchvision-0.19.0 triton-3.0.0 typing-inspection-0.4.2 uvicorn-0.38.0 uvloop-0.22.1 vllm-0.6.3.post1 watchfiles-1.1.1 websockets-15.0.1 xformers-0.0.27.post2 zipp-3.23.0
================================================================================
Abel Regime W Rollout Collection (vLLM Optimized)
================================================================================

Configuration:
  Merged model: checkpoints/abel_sft_merged
  Data path: data/gsm8k_train.jsonl
  Output path: data/abel_regime_w_rollouts.jsonl
  N samples: 500
  World size (GPUs): 1

Loading GSM8K training data from data/gsm8k_train.jsonl...
Using 500 examples for Regime W rollout collection

Built 8 Regime W arms (Wolfram + Maudlin styles)
Initializing vLLM engine with tensor parallelism...

`torch_dtype` is deprecated! Use `dtype` instead!
WARNING 11-19 07:26:19 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 11-19 07:26:19 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='checkpoints/abel_sft_merged', speculative_config=None, tokenizer='checkpoints/abel_sft_merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=checkpoints/abel_sft_merged, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 11-19 07:26:19 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.
INFO 11-19 07:26:19 selector.py:115] Using XFormers backend.
/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
INFO 11-19 07:26:22 model_runner.py:1056] Starting to load model checkpoints/abel_sft_merged...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/rl_training/collect_rollouts_abel_vllm.py", line 207, in <module>
[rank0]:     main()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/rl_training/collect_rollouts_abel_vllm.py", line 97, in main
[rank0]:     llm = LLM(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 177, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 573, in from_engine_args
[rank0]:     engine = cls(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 334, in __init__
[rank0]:     self.model_executor = executor_class(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/executor/executor_base.py", line 47, in __init__
[rank0]:     self._init_executor()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 40, in _init_executor
[rank0]:     self.driver_worker.load_model()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 183, in load_model
[rank0]:     self.model_runner.load_model()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1058, in load_model
[rank0]:     self.model = get_model(model_config=self.model_config,
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[rank0]:     return loader.load_model(model_config=model_config,
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 398, in load_model
[rank0]:     model = _initialize_model(model_config, self.load_config,
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 175, in _initialize_model
[rank0]:     return build_model(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py", line 160, in build_model
[rank0]:     return model_class(config=hf_config,
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 515, in __init__
[rank0]:     self.model = LlamaModel(config,
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 305, in __init__
[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 419, in make_layers
[rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/models/utils.py", line 420, in <listcomp>
[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 307, in <lambda>
[rank0]:     lambda prefix: LlamaDecoderLayer(config=config,
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 217, in __init__
[rank0]:     self.self_attn = LlamaAttention(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 134, in __init__
[rank0]:     self.q_size = self.num_heads * self.head_dim
[rank0]: TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'