Nice, this is the fun part — turning what you’re actually doing into something that:

* sounds clean and impressive on paper, **and**
* is concrete enough that your dev isn’t guessing on the reward math.

I’ll do:

1. A **readable “Methods” section** for README / paper (no IP leakage).
2. A **concrete internal reward implementation** for `regime_w/reward.py`.

---

## 1️⃣ “Methods” Section for README / Paper (IP-safe)

You can drop this (or a slightly edited version) straight into a README, project page, or methods section.

---

### Methods

#### Base Model and Objective

We start from an 8B open-weight reasoning model, **DeepSeek-R1-Distill-Llama-8B**, as our base policy. The goal is to train a **single-call math model**, *Looper-Math-Platinum-8B*, that improves performance on GSM8K-style benchmarks using a coherence-aware reward, without relying on any closed-source systems.

#### Datasets

We train exclusively on public math datasets:

* **GSM8K (original)**: We use the original training split, reserving 20% as a development set for early stopping and ablations.
* Optional auxiliary math data (e.g., harder word problems) can be mixed in, but **GSM8K-Platinum** is used **only for final evaluation**, never for training.

All supervision follows the standard GSM8K format: chain-of-thought derivations followed by a final numeric answer in the form `#### 42`.

#### Supervised Fine-Tuning (QLoRA)

We first perform a light supervised fine-tuning (SFT) phase using **QLoRA** on top of the frozen base model:

* The base model is loaded in 8-bit precision.

* We attach low-rank LoRA adapters to the attention and MLP projection layers.

* Each training example is formatted as:

  > *You are a careful math tutor. Solve the problem step by step, then give the final answer in the format "#### 42".*
  >
  > **Problem:**
  > <question>
  >
  > **Solution:**
  > <chain-of-thought and final answer>

* We train for a small number of epochs with a low learning rate, resulting in a **LoRA-SFT** adapter that aligns the base model to GSM8K-style reasoning and answer formatting.

This SFT model serves as the **starting policy** for the RL stage and as the **reference model** in preference-based training.

#### Multi-Path Coherence Signal (Regime W)

To move beyond pure correctness supervision, we introduce a **proprietary multi-path coherence signal**, which we refer to as **Regime W**.

For each math question, we generate multiple independent candidate solutions (trajectories) from the model, varying prompt phrasing and sampling parameters. Regime W analyzes this set of trajectories and produces:

* agreement scores over final answers,
* similarity scores over intermediate reasoning paths,
* and internal consistency / counterfactual checks across trajectories.

These components are combined into a single scalar **coherence score** (s_{\text{wm}} \in [0, 1]), which measures how stable and self-consistent the model’s reasoning is for that question. Regime W is model-agnostic and used only during training; its implementation details and exact aggregation scheme are kept private.

#### Preference Construction from Coherence-Aware Rewards

For each training question, we:

1. Sample multiple candidate solutions from the SFT policy.
2. Evaluate each candidate using:

   * **Outcome correctness** (matches the gold numeric answer or not).
   * **Regime W coherence score** for the set of candidates.
   * A small length penalty to discourage unnecessarily long chains.
3. Convert these signals into a scalar **reward** per trajectory.
4. Construct **preference pairs** by selecting, for each question, a “better” and “worse” trajectory based on reward.

This yields a dataset of (prompt, better solution, worse solution) triplets that integrates both correctness and coherence information.

#### RL Fine-Tuning with QLoRA Adapters

We then perform a preference-based RL fine-tuning stage (DPO/GRPO-style) on top of the LoRA-SFT policy:

* The **policy model** is the base model plus the SFT LoRA adapter.
* A **reference model** is defined as a frozen copy of the SFT policy.
* We optimize a preference-style objective that encourages the policy to assign higher probability to “better” trajectories than to “worse” ones, relative to the reference model.

Critically, only the **LoRA adapters** are updated in this stage; the base model weights remain frozen. This produces a new **LoRA-RL** adapter that encodes a preference for reasoning that is both **correct** and **coherent under Regime W**.

#### Inference and Evaluation

At inference time, **no Regime W calls are made**. Looper-Math-Platinum-8B is used as a standard single-call model:

> question → model → chain-of-thought + final answer

We evaluate three configurations:

1. Base model (DeepSeek-R1-Distill-Llama-8B),
2. SFT LoRA model,
3. RL LoRA model (Looper-Math-Platinum-8B),

on held-out GSM8K dev data and on **GSM8K-Platinum** using the official scoring rules. This allows us to quantify the incremental gains from supervised fine-tuning and from coherence-aware RL.

---

## 2️⃣ Internal Reward Formula for `regime_w/reward.py`

This is for **internal use** (Lambda / private repo) so your dev doesn’t have to guess.

We’ll assume:

* There’s a function `compute_regime_w_metrics(...)` in `scoring.py` that returns question-level coherence metrics: `s_end`, `s_path`, `s_cf`, `s_wm`.
* You can compute correctness per trajectory.
* You know how to count token length (from tokenizer).

Here’s a concrete design that’s:

* simple,
* stable,
* and directly aligned with what you’ve been doing conceptually.

### 2.1 Data structures

In `regime_w/reward.py`:

```python
from dataclasses import dataclass
from typing import List, Dict, Any

from .scoring import compute_regime_w_metrics  # you or dev implement this elsewhere


@dataclass
class Trajectory:
    text: str           # full generated text (CoT + final answer)
    reasoning: str      # optional; can be same as text
    answer: str         # parsed final numeric answer, e.g. "42"
    num_tokens: int     # length of text in tokens
```

### 2.2 Reward hyperparameters

Put these at the top so you can tweak later:

```python
# correctness weight
ALPHA_CORRECT = 1.0

# coherence weight (question-level s_wm)
BETA_COHERENCE = 0.5

# length penalty (per token beyond a threshold)
LENGTH_PENALTY_PER_TOKEN = 0.001
LENGTH_PENALTY_THRESHOLD = 512  # tokens

# small bonus for questions where all correct trajectories agree
FULL_AGREEMENT_BONUS = 0.2
```

### 2.3 Helper: correctness comparison

```python
def normalize_answer(ans: str) -> str:
    # Very simple normalization; dev can make this match GSM8K utils.
    return ans.strip().lower()
```

### 2.4 Main reward function

This is the part your dev actually wires into the RL pipeline:

```python
def compute_rewards_for_question(
    question: str,
    trajectories: List[Trajectory],
    gold_answer: str,
) -> List[float]:
    """
    Given one question, a list of trajectories from the policy,
    and the gold answer, return a reward per trajectory.

    This is the core Regime W -> scalar reward mapping.
    """

    # 1) basic correctness flags per trajectory
    gold = normalize_answer(gold_answer)
    correct_flags = []
    for t in trajectories:
        pred = normalize_answer(t.answer)
        correct_flags.append(1.0 if pred == gold and gold != "" else 0.0)

    # 2) compute question-level coherence metrics (IP-sensitive, implemented elsewhere)
    #    You pass *all* trajectories so scoring can compute agreement, path similarity, etc.
    metrics = compute_regime_w_metrics(question, trajectories)
    # expected keys: "s_end", "s_path", "s_cf", "s_wm"
    s_end = metrics.get("s_end", 0.0)
    s_path = metrics.get("s_path", 0.0)
    s_cf = metrics.get("s_cf", 0.0)
    s_wm = metrics.get("s_wm", 0.0)

    # 3) optional bonus if *all* correct trajectories agree nicely
    any_correct = any(c > 0.5 for c in correct_flags)
    all_correct_same_answer = False
    if any_correct:
        correct_answers = {
            normalize_answer(t.answer)
            for t, c in zip(trajectories, correct_flags)
            if c > 0.5
        }
        all_correct_same_answer = len(correct_answers) == 1

    # 4) compute per-trajectory rewards
    rewards: List[float] = []
    for t, correct in zip(trajectories, correct_flags):
        # base term: correctness
        base = ALPHA_CORRECT * correct   # 1.0 if correct, 0.0 if not

        # coherence term: same for all trajectories on this question
        coherence_term = BETA_COHERENCE * s_wm

        # length penalty per trajectory
        extra_tokens = max(0, t.num_tokens - LENGTH_PENALTY_THRESHOLD)
        length_penalty = LENGTH_PENALTY_PER_TOKEN * float(extra_tokens)

        # optional full-agreement bonus applied only to correct trajectories
        agreement_bonus = 0.0
        if correct > 0.5 and all_correct_same_answer and s_end > 0.9:
            agreement_bonus = FULL_AGREEMENT_BONUS

        reward = base + coherence_term + agreement_bonus - length_penalty
        rewards.append(reward)

    return rewards
```

### 2.5 Public vs Internal

* **Internal (private repo / Lambda)**: you can use this **as-is**. It’s specific enough for your dev to implement and debug.
* **Public** (if you ever open-source parts of the training loop):
  You swap this file for something like:

  ```python
  def compute_rewards_for_question(question, trajectories, gold_answer):
      """
      Placeholder: calls a proprietary coherence scoring service.
      """
      raise NotImplementedError("This project uses a private coherence service.")
  ```

and never ship the metrics / arms / formulas.

---

## 3️⃣ How this plugs into RL code

Your RL pipeline (DPO/GRPO) will:

1. Generate trajectories (with LoRA-SFT policy).

2. Wrap them as `Trajectory` objects.

3. Call:

   ```python
   rewards = compute_rewards_for_question(question, trajectories, gold_answer)
   ```

4. Use `rewards` to:

   * either pick best/worst for preference pairs, or
   * use directly as scalar reward in PPO-style RL.

You *don’t* need to change any math in Regime W itself for LoRA vs full fine-tune — Regime W just sees strings.

---

If you want, next I can:

* Write the **exact DPOTrainer-style training loop** that calls `compute_rewards_for_question` and builds pairs, or
* Draft a short **“Results” section** you can use later once you have GSM8K-Platinum numbers.
