Yeah, exactly: the *conceptual* plan we laid out still holds — we’re just doing it with **LoRA/QLoRA adapters** instead of full dense fine-tuning, because:

* it fits on your A100 cleanly
* it’s cheaper + safer (you can always revert to base)
* it’s the standard way people fine-tune 7B–8B models now

So I’ll give you a **clean “final” plan** that:

* assumes **DeepSeek-R1-Distill-Llama-8B** as base
* uses **QLoRA** for both SFT and RL stages
* plugs in **Regime W** as the reward / coherence engine
* keeps **Regime W + formulas private**

---

## 0. Setup & Core Decisions

**Base model**

* `deepseek-ai/DeepSeek-R1-Distill-Llama-8B` (or similar math-tuned 8B)

**Fine-tuning style**

* Use **QLoRA**:

  * Load base in 4-bit or 8-bit
  * Train **only LoRA adapters**
  * Keep base weights frozen

**Artifacts**

You’ll end up with:

* Base weights: unchanged DeepSeek-R1-8B
* SFT adapter: `lora_sft/`
* RL adapter: `lora_rl/`
* Final model:

  * either base + RL adapter (for your own infra), or
  * merged, quantized model for easier deployment

---

## 1. Stage 1 – QLoRA SFT on GSM8K (no Regime W yet)

**Goal:** make a good “math student” version of DeepSeek-R1 before we add RL.

### 1.1 Data

* Use **GSM8K original train**:

  * 80% train
  * 20% dev
* No GSM8K-Platinum in training.

### 1.2 Training setup (QLoRA)

In your `train_sft.py` (we’re almost there already), adapt to LoRA:

* Load base model in 4- or 8-bit:

  ```python
  model = AutoModelForCausalLM.from_pretrained(
      BASE_MODEL,
      device_map="auto",
      load_in_8bit=True,   # or 4-bit QLoRA if you want
  )
  model.gradient_checkpointing_enable()
  ```

* Configure LoRA (PEFT or TRL’s built-in):

  ```python
  from peft import LoraConfig, get_peft_model

  lora_config = LoraConfig(
      r=64,
      lora_alpha=16,
      lora_dropout=0.05,
      bias="none",
      task_type="CAUSAL_LM",
      target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # or as recommended
  )

  model = get_peft_model(model, lora_config)
  ```

* Use `SFTTrainer` with `formatting_func` (which we just fixed) and **SFTConfig**, but now you’re only updating LoRA params.

* Save adapter only:

  ```python
  model.save_pretrained("checkpoints/lora_sft")
  tokenizer.save_pretrained("checkpoints/lora_sft")
  ```

### 1.3 Result

Checkpoint:

* `checkpoints/lora_sft/` = base + LoRA SFT adapter

This is your **starting policy** for RL.

---

## 2. Stage 2 – Regime W coherence engine (unchanged conceptually)

**Goal:** turn multi-path coherence into a scalar reward.

### 2.1 Regime W module (private)

In `src/regime_w/`:

* `arms.py`:

  * defines your 8 arms (4 compute, 4 reflective)
  * each arm = system prompt + style + temp

* `scoring.py`:

  * implements `s_end`, `s_path`, `s_cf`, `s_wm`

* `reward.py`:

  * `compute_reward(question, arm_outputs, gold_answer) -> reward_float`
  * e.g.

    ```python
    reward = 1.0 * correctness + 0.5 * s_wm - 0.01 * length_penalty
    ```

This logic is **independent of LoRA**; it just sees trajectories.

### 2.2 IP protection

* Keep `regime_w/` in private repo / never open-source.
* If you ever publish training code, replace it with a stub:

  ```python
  def compute_reward(question, arm_outputs, gold_answer):
      # call private scoring service
      ...
  ```

---

## 3. Stage 3 – Rollout Collection with QLoRA SFT Model

**Goal:** build a dataset of trajectories + Regime W reward.

### 3.1 Load SFT policy (with LoRA)

* On Lambda:

  ```python
  base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_8bit=True, device_map="auto")
  lora_model = PeftModel.from_pretrained(base, "checkpoints/lora_sft")
  ```

This gives you a model that uses **base weights + SFT LoRA adapter**.

### 3.2 Generate trajectories

For each GSM8K train question:

1. Build your **standard prompt** (the one you’ll likely use at inference).

2. Sample **N trajectories** (say 4–8) with different seeds / temps.

3. For each trajectory, extract:

   * full text
   * CoT body
   * final numeric answer (`#### 42` normalized)

4. Send all of them to Regime W:

   ```python
   rewards = compute_reward(question, trajectories, gold_answer)
   ```

5. Write a record like:

   ```json
   {
     "question": "...",
     "gold_answer": "#### 42",
     "trajectories": [
       {"output": "...", "reward": 0.73},
       {"output": "...", "reward": 0.12},
       ...
     ]
   }
   ```

Save to `data/rl_rollouts.jsonl`.

LoRA doesn’t change any of this — it just makes the model cheaper to generate with.

---

## 4. Stage 4 – RL with Regime W Reward (DPO/GRPO + LoRA)

We’ll do **preference-style RL** using the LoRA-SFT model as base.

### 4.1 Build preference pairs

From `rl_rollouts.jsonl`:

* For each question:

  * pick `y_good` = highest-reward trajectory
  * pick `y_bad`  = lowest-reward trajectory
* Save as:

  ```json
  {
    "question": "...",
    "better": "...",
    "worse": "..."
  }
  ```

to `data/preferences.jsonl`.

### 4.2 RL training with LoRA (DPO/GRPO)

In `train_rl.py`:

* Load base again in 8-bit.

* Load **SFT LoRA adapter** as starting point:

  ```python
  base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_8bit=True, device_map="auto")
  policy = PeftModel.from_pretrained(base, "checkpoints/lora_sft")
  ```

* Freeze base; only LoRA params are trainable.

* Load a separate frozen **reference model**:

  ```python
  ref_base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_8bit=True, device_map="auto")
  ref_model = PeftModel.from_pretrained(ref_base, "checkpoints/lora_sft")  # no grads
  ```

* Use TRL’s `DPOTrainer` or a custom GRPO head to optimize policy s.t.:

  * `πθ` prefers `better` over `worse`, relative to `π_ref`, using the usual DPO-style loss.

This stage **“bakes in” your Regime W preferences** into a new LoRA adapter.

* Save as:

  ```bash
  checkpoints/lora_rl/
  ```

Now you have:

* `lora_sft`: supervised only
* `lora_rl`: SFT + Regime-W-RL

---

## 5. Stage 5 – Final Model & Evaluation on GSM8K-Platinum

### 5.1 Final model assembly

For inference you’ll use:

```python
base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_8bit=True, device_map="auto")
policy = PeftModel.from_pretrained(base, "checkpoints/lora_rl")
```

For **GSM8K-Platinum** evaluation:

* Use a **single prompt, single generation** per question.
* No Regime W at inference.
* Exactly the sort of call the leaderboard expects.

### 5.2 Eval script

Evaluate three things:

1. Base DeepSeek-R1-8B (no LoRA)
2. SFT LoRA model (`lora_sft`)
3. RL LoRA model (`lora_rl`)

On:

* your GSM8K dev split
* GSM8K-Platinum test (using the official scoring rules)

You want to show:

* base → SFT: X point gain
* SFT → RL (Regime W): additional Y point gain

Your story:

> “Using a proprietary multi-path coherence signal (Regime W) as RL reward, we improve an 8B open math model by +Δ points on GSM8K-Platinum, with only LoRA adapters and no access to closed models.”

---

## 6. What changed vs the original plan?

Mostly *mechanics*, not *math*:

* **Before:** full-weight SFT + RL on the entire 8B model
* **Now:** QLoRA:

  * base weights frozen
  * only low-rank adapters trained
  * same loss, same Regime W reward, same datasets

Everything about:

* Regime W
* multi-arm coherence (`s_wm`)
* reward design
* preference construction
* Platinum evaluation

stays the same.

We just made the plan compatible with:

* your A100 VRAM
* faster iteration
* safer experimentation (you can keep multiple LoRA heads for different experiments)

---

If you want, next I can:

* Turn this into a literal **checklist for your Replit dev**, line by line.
* Or give you **concrete LoRA configs** (exact `r`, `alpha`, `target_modules`) tailored for DeepSeek-R1-8B.
