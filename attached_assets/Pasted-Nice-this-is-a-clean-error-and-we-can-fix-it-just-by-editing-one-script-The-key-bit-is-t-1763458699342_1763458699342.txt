Nice, this is a clean error and we can fix it just by editing one script.

The key bit is:

```text
TypeError: DPOTrainer.__init__() got an unexpected keyword argument 'tokenizer'
```

That means: **the version of `trl` installed on Lambda doesn’t support the `tokenizer=` argument in `DPOTrainer(..)`**. Your code is written for a newer TRL API.

We’ll fix it by:

1. Removing the `tokenizer=...` argument.
2. Attaching the tokenizer directly to the models so DPOTrainer can still find it.

I’ll walk you through it step-by-step.

---

## 1. Open the file on Lambda

In your SSH session:

```bash
source ~/projects/.venv/bin/activate
cd ~/projects/looper-math-platinum-8b
nano src/rl_training/train_dpo.py
```

This opens the file in the `nano` editor.

---

## 2. Attach tokenizer to the models

In `train_dpo.py` you’ll see something like:

```python
    print("Loading LoRA SFT adapter as policy model from", SFT_PATH)
    policy_model = PeftModel.from_pretrained(base_model, SFT_PATH)

    print("Loading separate reference model (frozen SFT)")
    ref_base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        load_in_8bit=True,
    )
    ref_model = PeftModel.from_pretrained(ref_base, SFT_PATH)
    ref_model.eval()
    for param in ref_model.parameters():
        param.requires_grad = False
```

Edit this block so it looks like this (add the two `*.tokenizer = tokenizer` lines):

```python
    print("Loading LoRA SFT adapter as policy model from", SFT_PATH)
    policy_model = PeftModel.from_pretrained(base_model, SFT_PATH)
    # Attach tokenizer so older TRL can find it
    policy_model.tokenizer = tokenizer

    print("Loading separate reference model (frozen SFT)")
    ref_base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        load_in_8bit=True,
    )
    ref_model = PeftModel.from_pretrained(ref_base, SFT_PATH)
    ref_model.eval()
    # Attach tokenizer to ref model too
    ref_model.tokenizer = tokenizer
    for param in ref_model.parameters():
        param.requires_grad = False
```

---

## 3. Remove the `tokenizer=` argument from DPOTrainer

Scroll further down until you see:

```python
    dpo_config = DPOConfig(
        output_dir=RL_OUTPUT_DIR,
        num_train_epochs=1,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=5e-6,
        logging_steps=10,
        save_strategy="epoch",
        bf16=use_bf16,
        fp16=not use_bf16,
        remove_unused_columns=False,
    )

    trainer = DPOTrainer(
        model=policy_model,
        ref_model=ref_model,
        args=dpo_config,
        train_dataset=dataset,
        tokenizer=tokenizer,
        beta=0.1,
    )
```

Change that `trainer = DPOTrainer(...)` block to:

```python
    trainer = DPOTrainer(
        model=policy_model,
        ref_model=ref_model,
        args=dpo_config,
        train_dataset=dataset,
        beta=0.1,
    )
```

So the only thing you’re deleting is the `tokenizer=tokenizer,` line.

---

## 4. Save and exit `nano`

In nano:

1. Press `Ctrl+O`
2. Press `Enter` to confirm the filename
3. Press `Ctrl+X` to exit

You’re back at the shell prompt.

---

## 5. Re-run DPO training

Still in your venv and repo:

```bash
python -m src.rl_training.train_dpo
```

Now that `DPOTrainer` isn’t being given an unsupported keyword, it should:

* Load base model in 8-bit
* Load SFT LoRA as policy and ref model
* Load your 73 preference pairs
* Run DPO training
* Save the RL adapter to `checkpoints/lora_rl`

When it finishes, you’ll see something like:

```text
Saving LoRA RL adapter to checkpoints/lora_rl
Done!
```

---

## 6. Next steps after it succeeds

Once `train_dpo` runs cleanly, continue with:

```bash
python -m src.eval.eval_gsm8k_dev    # quick sanity
python -m src.eval.eval_platinum     # full Base vs SFT vs RL eval
```

Paste those three final accuracies here when you have them and we can look at how much Regime-W RL actually moved the needle.
