
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.

Starting Abel SFT (LoRA) training...

Per-device effective batch size: 4
World size (GPUs): 8
Global effective batch size: 32
Total training steps: 233

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.
  0%|                                                                                           | 0/234 [00:00<?, ?it/s][rank4]: Traceback (most recent call last):
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank4]:     main()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank4]:     trainer.train()
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank4]:     return super().training_step(*args, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank4]:     loss.backward(**kwargs)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank4]:     return user_fn(self, *args)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank4]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank4]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank6]:     main()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank6]:     trainer.train()
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank6]:     return super().training_step(*args, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank6]:     loss.backward(**kwargs)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank6]:     return user_fn(self, *args)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank6]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank6]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank7]:     main()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank7]:     trainer.train()
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank7]:     return super().training_step(*args, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank7]:     loss.backward(**kwargs)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank7]:     return user_fn(self, *args)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank7]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank7]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank2]:     main()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank2]:     trainer.train()
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank2]:     return super().training_step(*args, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank2]:     return user_fn(self, *args)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank2]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank2]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank3]:     main()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank3]:     trainer.train()
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank3]:     return super().training_step(*args, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank3]:     loss.backward(**kwargs)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank3]:     return user_fn(self, *args)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank3]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank3]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank0]:     main()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank0]:     return super().training_step(*args, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank0]:     return user_fn(self, *args)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank0]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank0]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank5]:     main()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank5]:     trainer.train()
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank5]:     return super().training_step(*args, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank5]:     loss.backward(**kwargs)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank5]:     return user_fn(self, *args)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank5]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank5]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 166, in <module>
[rank1]:     main()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/src/baseline_sft/train_sft_abel.py", line 152, in main
[rank1]:     trainer.train()
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 1190, in training_step
[rank1]:     return super().training_step(*args, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/trainer.py", line 4071, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 2740, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/_tensor.py", line 625, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/function.py", line 315, in apply
[rank1]:     return user_fn(self, *args)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 319, in backward
[rank1]:     torch.autograd.backward(outputs_with_grad, args_with_grad)
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes. or try to use _set_static_graph() as a workaround if this module graph does not change during training loop.2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases in default. You can try to use _set_static_graph() as a workaround if your module graph does not change over iterations.
[rank1]: Parameter at index 447 with name base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration.
  0%|                                                                                           | 0/234 [00:01<?, ?it/s]
[rank0]:[W1119 06:23:11.358062816 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1119 06:23:13.511000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31440 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31442 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31443 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31444 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31445 closing signal SIGTERM
W1119 06:23:13.512000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31446 closing signal SIGTERM
W1119 06:23:13.513000 31340 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 31447 closing signal SIGTERM
E1119 06:23:14.128000 31340 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 1 (pid: 31441) of binary: /home/ubuntu/looper-math-platinum-8b/.venv/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/looper-math-platinum-8b/.venv/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1226, in launch_command
    multi_gpu_launcher(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/accelerate/commands/launch.py", line 853, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
src/baseline_sft/train_sft_abel.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-19_06:23:13
  host      : 192-222-54-90
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 31441)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$