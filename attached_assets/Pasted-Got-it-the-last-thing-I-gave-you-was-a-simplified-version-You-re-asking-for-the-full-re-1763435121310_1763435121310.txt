Got it — the last thing I gave you was a **simplified** version.

You’re asking for the **full “real thing”** we talked about before:

> Regime W with multiple arms (W/M), coherence scoring,
> rollouts over arms → `compute_reward(question, arm_outputs, gold)` → PPO RL → eval.

So let’s do *exactly that*, step by step, with copy-pasteable files and minimal changes to what you already have.

I’ll assume you’re already comfortable getting onto the box:

```bash
ssh -i "$HOME/Downloads/lambda_a100_key.pem" ubuntu@129.146.54.95
source ~/projects/.venv/bin/activate
cd ~/projects/looper-math-platinum-8b
```

---

## 1️⃣ Regime W: arms + scoring + reward + demo

### 1.1 `arms.py` – W/M arms

```bash
mkdir -p src/regime_w
nano src/regime_w/arms.py
```

Paste:

```python
from dataclasses import dataclass
from typing import List


@dataclass
class ArmSpec:
    name: str
    system_prompt: str
    temp: float
    top_p: float


def build_all_arms() -> List[ArmSpec]:
    arms: List[ArmSpec] = []

    wolfram_system = (
        "You are a precise math reasoner. "
        "Solve the problem step by step, keep reasoning linear and explicit. "
        "Avoid extra commentary. End with '#### final_answer'."
    )
    for i, t in enumerate([0.2, 0.3, 0.4, 0.5]):
        arms.append(ArmSpec(
            name=f"wolfram_{i}",
            system_prompt=wolfram_system,
            temp=t,
            top_p=0.95,
        ))

    maudlin_system = (
        "You are a reflective math reasoner. "
        "Explain your reasoning and highlight any steps that might be fragile. "
        "End with '#### final_answer'."
    )
    for i, t in enumerate([0.2, 0.3, 0.4, 0.5]):
        arms.append(ArmSpec(
            name=f"maudlin_{i}",
            system_prompt=maudlin_system,
            temp=t,
            top_p=0.9,
        ))

    return arms
```

Save (`Ctrl+O`, Enter, `Ctrl+X`).

---

### 1.2 `scoring.py` – s_end, s_path, s_cf, s_wm

```bash
nano src/regime_w/scoring.py
```

Overwrite with:

```python
from typing import List, Dict
import math


def s_end_for_question(answers: List[str]) -> float:
    """
    Pairwise agreement on final answers across arms.
    """
    n = len(answers)
    if n < 2:
        return 1.0 if n == 1 else 0.0

    agree = 0
    total = 0
    for i in range(n):
        for j in range(i + 1, n):
            total += 1
            if answers[i] == answers[j]:
                agree += 1
    return agree / total if total > 0 else 0.0


def s_path_for_question(reasonings: List[str]) -> float:
    """
    Coarse path coherence: similar lengths => higher score.
    """
    if not reasonings:
        return 0.0

    lengths = [len(r.split()) for r in reasonings]
    avg = sum(lengths) / len(lengths)
    if avg <= 0:
        return 0.0

    var = sum((l - avg) ** 2 for l in lengths) / len(lengths)
    score = 1.0 / (1.0 + var / (avg + 1e-6))
    return max(0.0, min(1.0, score))


def s_cf_for_question(arm_outputs: List[Dict]) -> float:
    """
    Placeholder counterfactual/cross-check score.
    You can later plug in your real CHSH-ish logic here.
    """
    return 0.5


def s_wm_for_question(s_end: float, s_path: float, s_cf: float) -> float:
    """
    Combined Regime W coherence score.
    """
    return 0.4 * s_end + 0.3 * s_path + 0.3 * s_cf
```

---

### 1.3 `reward.py` – Regime W reward wrapper

```bash
nano src/regime_w/reward.py
```

Paste:

```python
from typing import List, Dict, Optional
import re

from .scoring import (
    s_end_for_question,
    s_path_for_question,
    s_cf_for_question,
    s_wm_for_question,
)


def extract_answer(text: str) -> str:
    """
    Prefer the number after '####'; otherwise last number in text.
    """
    marker = "####"
    if marker in text:
        tail = text.split(marker)[-1]
        m = re.search(r"-?\d+\.?\d*", tail)
        if m:
            return m.group(0).strip()

    nums = re.findall(r"-?\d+\.?\d*", text)
    if nums:
        return nums[-1].strip()

    return text.strip()


def compute_regime_w_scores(
    question: str,
    arm_outputs: List[Dict],
    gold_answer: Optional[str] = None,
) -> Dict:
    """
    arm_outputs: list of dicts with keys:
      - "answer"
      - "reasoning"
      - "full_text"
      - "arm_name" (optional, for logging)
    """
    answers = [a["answer"] for a in arm_outputs]
    reasonings = [a["reasoning"] for a in arm_outputs]

    s_end = s_end_for_question(answers)
    s_path = s_path_for_question(reasonings)
    s_cf = s_cf_for_question(arm_outputs)
    s_wm = s_wm_for_question(s_end, s_path, s_cf)

    correct_flags: List[bool] = []
    if gold_answer is not None:
        gold = extract_answer(gold_answer)
        for ans in answers:
            correct_flags.append(extract_answer(ans) == gold)

    return {
        "s_end": s_end,
        "s_path": s_path,
        "s_cf": s_cf,
        "s_wm": s_wm,
        "correct_flags": correct_flags,
    }


def compute_reward(
    question: str,
    arm_outputs: List[Dict],
    gold_answer: Optional[str] = None,
) -> float:
    """
    Regime W scalar reward:
      - base: correctness across arms
      - + coherence bonus
      - + mild length penalty
    """
    scores = compute_regime_w_scores(question, arm_outputs, gold_answer)
    s_wm = scores["s_wm"]
    correct_flags = scores["correct_flags"]

    correct_any = any(correct_flags) if correct_flags else False

    base = 1.0 if correct_any else -0.5
    coherence_bonus = 0.7 * s_wm

    avg_len = sum(
        len(a["full_text"].split()) for a in arm_outputs
    ) / max(1, len(arm_outputs))
    len_penalty = -0.001 * max(0, avg_len - 400)

    reward = base + coherence_bonus + len_penalty
    return reward
```

---

### 1.4 `demo.py` – sanity check

```bash
nano src/regime_w/demo.py
```

Paste:

```python
from .reward import compute_reward


def main():
    question = "If you have 3 apples and buy 4 more, how many apples?"
    gold = "#### 7"
    arm_outputs = [
        {
            "arm_name": "wolfram_0",
            "full_text": "3 + 4 = 7\n#### 7",
            "reasoning": "3 + 4 = 7",
            "answer": "#### 7",
        },
        {
            "arm_name": "maudlin_0",
            "full_text": "Start with 3 apples, add 4, total 7.\n#### 7",
            "reasoning": "Start with 3 apples, add 4",
            "answer": "#### 7",
        },
    ]

    r = compute_reward(question, arm_outputs, gold)
    print("Regime W demo reward:", r)


if __name__ == "__main__":
    main()
```

Run it:

```bash
python -m src.regime_w.demo
```

You should see a numeric reward; if no error, Regime W is wired.

---

## 2️⃣ Multi-arm rollout collection (the “Phase 3” we discussed)

Now we’ll write a collector that:

* Loads GSM8K train
* Loads your LoRA SFT model
* Builds **8 arms** via `build_all_arms()`
* For each question:

  * Generates one sample per arm
  * Assembles `arm_outputs`
  * Calls `compute_reward`
  * Writes a JSONL record with:

    * question, gold_answer
    * arm_outputs
    * reward

Create:

```bash
mkdir -p src/rl
nano src/rl/collect_rollouts_regime_w.py
```

Paste:

```python
import json
from typing import List, Dict

from transformers import AutoTokenizer
from peft import AutoPeftModelForCausalLM

from src.regime_w.arms import build_all_arms
from src.regime_w.reward import compute_reward, extract_answer

DATA_PATH = "data/gsm8k_train.jsonl"
SFT_PATH = "checkpoints/sft_lora"
OUT_PATH = "data/regime_w_rollouts.jsonl"


def load_gsm8k(path: str) -> List[Dict]:
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line))
    return data


def build_prompt(system_prompt: str, question: str) -> str:
    return (
        system_prompt
        + "\n\n"
        "Solve the problem step by step and end with the final answer "
        "in the format '#### 42'.\n\n"
        f"Problem:\n{question}\n\nSolution:\n"
    )


def main():
    print("Loading GSM8K train data...")
    dataset = load_gsm8k(DATA_PATH)
    # Start small; you can increase later
    dataset = dataset[:500]
    print(f"Using {len(dataset)} examples for Regime W rollout collection.")

    print("Loading LoRA SFT model from", SFT_PATH)
    tokenizer = AutoTokenizer.from_pretrained(SFT_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoPeftModelForCausalLM.from_pretrained(
        SFT_PATH,
        device_map="auto",
    )

    arms = build_all_arms()
    print(f"Built {len(arms)} arms.")

    with open(OUT_PATH, "w", encoding="utf-8") as out_f:
        for idx, ex in enumerate(dataset):
            q = ex["question"]
            gold = ex["answer"]

            arm_outputs = []

            for arm in arms:
                prompt = build_prompt(arm.system_prompt, q)
                inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
                gen = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=arm.temp,
                    top_p=arm.top_p,
                    pad_token_id=tokenizer.eos_token_id,
                )
                text = tokenizer.decode(gen[0], skip_special_tokens=True)
                ans = extract_answer(text)

                arm_outputs.append({
                    "arm_name": arm.name,
                    "full_text": text,
                    "reasoning": text,
                    "answer": ans,
                })

            reward = compute_reward(q, arm_outputs, gold)

            record = {
                "question": q,
                "gold_answer": gold,
                "arm_outputs": arm_outputs,
                "reward": reward,
            }
            out_f.write(json.dumps(record) + "\n")

            if (idx + 1) % 20 == 0:
                print(f"  Collected {idx+1}/{len(dataset)} rollouts")

    print("Saved Regime W rollouts to", OUT_PATH)


if __name__ == "__main__":
    main()
```

Run:

```bash
python -m src.rl.collect_rollouts_regime_w
```

Let it finish. That’s the **real multi-arm Regime W rollout collection** we planned.

---

## 3️⃣ PPO RL using Regime W rollouts

To stay faithful to TRL’s design and still use your rollouts, we’ll:

* Treat each question as **one PPO sample** by picking **one arm’s response** per question.
* Use the **same scalar reward** (Regime W reward for that multi-arm set) for that sample.
* (You can later change strategy: best arm, random arm, or mixture.)

Create:

```bash
nano src/rl/train_ppo_regime_w_from_rollouts.py
```

Paste:

```python
import json
import random
from typing import List, Dict

from transformers import AutoTokenizer
from peft import AutoPeftModelForCausalLM
from trl import PPOTrainer, PPOConfig

from src.regime_w.reward import extract_answer

SFT_PATH = "checkpoints/sft_lora"
ROLLOUT_PATH = "data/regime_w_rollouts.jsonl"
RL_OUTPUT_DIR = "checkpoints/ppo_regime_w_full"


def load_rollouts(path: str) -> List[Dict]:
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line))
    return data


def build_prompt(question: str) -> str:
    return (
        "You are a careful math tutor. Solve the problem step-by-step, "
        "then give the final answer in the format '#### 42'.\n\n"
        f"Problem:\n{question}\n\nSolution:\n"
    )


def main():
    print("Loading Regime W rollouts from", ROLLOUT_PATH)
    rollouts = load_rollouts(ROLLOUT_PATH)
    random.seed(42)
    random.shuffle(rollouts)

    print(f"Loaded {len(rollouts)} rollout records.")

    print("Loading SFT LoRA model from", SFT_PATH)
    tokenizer = AutoTokenizer.from_pretrained(SFT_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoPeftModelForCausalLM.from_pretrained(
        SFT_PATH,
        device_map="auto",
    )

    config = PPOConfig(
        model_name=SFT_PATH,
        learning_rate=5e-6,
        batch_size=8,
        mini_batch_size=4,
        gradient_accumulation_steps=1,
        target_kl=0.1,
        adap_kl_ctrl=True,
        max_grad_norm=1.0,
        output_dir=RL_OUTPUT_DIR,
        log_with=None,
    )

    ppo_trainer = PPOTrainer(
        config=config,
        model=model,
        tokenizer=tokenizer,
    )

    print("Starting PPO training loop using offline Regime W rollouts...")
    num_epochs = 1

    for epoch in range(num_epochs):
        print(f"\n=== PPO Epoch {epoch+1}/{num_epochs} ===")
        for start in range(0, len(rollouts), config.batch_size):
            batch = rollouts[start : start + config.batch_size]
            if not batch:
                break

            queries = []
            responses = []
            rewards = []

            for rec in batch:
                q = rec["question"]
                reward = rec["reward"]
                arm_outputs = rec["arm_outputs"]

                # Choose one arm's output as the "response" for PPO
                chosen = random.choice(arm_outputs)
                resp_text = chosen["full_text"]

                queries.append(build_prompt(q))
                responses.append(resp_text)
                rewards.append(reward)

            stats = ppo_trainer.step(queries, responses, rewards)

            step_idx = start // config.batch_size
            if step_idx % 10 == 0:
                avg_reward = sum(rewards) / len(rewards)
                print(
                    f"  Step {step_idx}, "
                    f"batch_size={len(batch)}, "
                    f"avg_reward={avg_reward:.3f}, "
                    f"kl={stats.get('kl', 0):.4f}"
                )

    print("Saving PPO + Regime W (full) model to", RL_OUTPUT_DIR)
    ppo_trainer.model.save_pretrained(RL_OUTPUT_DIR)
    tokenizer.save_pretrained(RL_OUTPUT_DIR)
    print("Done.")


if __name__ == "__main__":
    main()
```

Run:

```bash
python -m src.rl.train_ppo_regime_w_from_rollouts
```

That’s your **“real thing”**:

* Multi-arm W/M sampling
* Regime W coherence + correctness reward
* Offline rollouts → PPO training.

---

## 4️⃣ Eval: Base vs SFT vs Regime W PPO

You already have an eval script. Add one more model:

```bash
nano src/eval/eval_gsm8k_dev.py
```

Add this function:

```python
def eval_regime_w_ppo_full_model():
    label = "Regime W PPO full model"
    model_path = "checkpoints/ppo_regime_w_full"

    print(f"Evaluating {label} from {model_path}")
    from peft import AutoPeftModelForCausalLM

    model = AutoPeftModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    dev = load_dev(DEV_PATH)
    dev = dev[:50]

    correct = 0
    for i, ex in enumerate(dev):
        q = ex["question"]
        gold = ex["answer"]

        prompt = build_prompt(q)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        gen = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.6,
            top_p=0.9,
        )
        text = tokenizer.decode(gen[0], skip_special_tokens=True)
        pred = extract_answer(text)
        gold_num = extract_answer(gold)

        if pred == gold_num:
            correct += 1

        if (i + 1) % 10 == 0:
            print(f"  Regime W PPO full progress: {i+1}/{len(dev)}")

    acc = correct / len(dev)
    print(f"{label} accuracy on dev ({len(dev)} examples): {acc:.3f} ({correct}/{len(dev)})\n")
```

Then make `main()` call all three:

```python
def main():
    print("="*60)
    print("GSM8K Dev: Base vs LoRA SFT vs Regime W PPO (full)")
    print("="*60)

    eval_base_model()
    eval_lora_sft_model()
    eval_regime_w_ppo_full_model()

    print("="*60)
    print("Evaluation complete!")
```

Run:

```bash
python -m src.eval.eval_gsm8k_dev
```

You’ll get:

* Base accuracy
* LoRA SFT accuracy
* Regime W PPO (full) accuracy

---

### That *is* the “real thing” we discussed:

* ✅ SFT with QLoRA
* ✅ Regime W with arms + coherence scores
* ✅ Rollout collection over W/M arms
* ✅ PPO RL using Regime W reward
* ✅ Eval with Base vs SFT vs Regime W PPO

If you paste those three accuracy lines, I’ll help interpret whether RL is actually helping and what to tweak next.
