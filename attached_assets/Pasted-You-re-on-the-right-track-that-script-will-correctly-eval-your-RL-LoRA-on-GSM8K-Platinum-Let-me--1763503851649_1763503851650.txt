You’re on the right track: that script *will* correctly eval your RL LoRA on GSM8K-Platinum. Let me first sanity-check it, then I’ll give you a drop-in version that:

* Logs **every example** to a JSONL file
* Can **resume** from where it left off
* Keeps running accuracy stats so you can analyze gaps later

---

## 1. Is your current script “correct” as a test?

Quick review of what you wrote:

* **Dataset**:

  ```python
  platinum_test = load_dataset("madrylab/gsm8k-platinum", "main", split="test")
  ```

  ✅ That’s the right HF dataset + split.

* **Model loading**:

  ```python
  base_model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
  lora_path = "checkpoints/lora_rl"

  tokenizer = AutoTokenizer.from_pretrained(base_model_name)

  base_model = AutoModelForCausalLM.from_pretrained(
      base_model_name,
      device_map="auto",
      load_in_8bit=True,
  )
  model = PeftModel.from_pretrained(base_model, lora_path)
  model.eval()
  ```

  ✅ Correct: base DeepSeek R1 8B in 8-bit + LoRA RL adapter.
  Small optional nicety: set `tokenizer.pad_token = tokenizer.eos_token` if it complains, but not required.

* **Prompt format**:

  ```python
  "You are a careful math tutor... '#### 42'..."
  ```

  ✅ Matches how you trained SFT/RL. Good.

* **Answer extraction + normalization**:

  * `extract_answer` looks for `####` marker and pulls a numeric token.
  * `normalize_answer` lowercases, strips commas, collapses `42.0 → 42`.
    ✅ Solid, consistent with GSM8K norms.

* **Eval loop**:

  * Iterates over whole `platinum_test`
  * Computes `pred` and `gold_num`
  * Counts correct when `pred == gold_num and pred != ""`
    ✅ That’s reasonable.

So: **yes, as a one-shot “evaluate RL on all Platinum test” this is logically fine.**

What it *doesn’t* do yet is:

* Save per-example records
* Support resuming after a crash/disconnect
* Let you re-analyze errors later without re-running models

Let’s fix that.

---

## 2. Drop-in upgraded script with resume + logging

Here’s a version you can just replace your current file with. It:

* Writes every example to `outputs/rl_platinum_eval.jsonl`
* If that file already exists, it:

  * Reads it
  * Computes how many examples are already done
  * Computes how many of those were correct
  * **Resumes from the next index**
* Logs enough fields to analyze later:

  * index
  * question
  * gold answer (raw)
  * extracted + normalized gold
  * model raw text (answer region)
  * extracted + normalized prediction
  * correct flag

```python
import os
import json
import re
from typing import List, Dict

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel


OUT_DIR = "outputs"
OUT_PATH = os.path.join(OUT_DIR, "rl_platinum_eval.jsonl")


def normalize_answer(ans: str) -> str:
    """
    Normalize answer for comparison.
    - Strip whitespace
    - Lowercase
    - Remove trailing .0 from decimals (42.0 -> 42)
    - Handle commas in numbers (1,000 -> 1000)
    """
    ans = ans.strip().lower()
    ans = ans.replace(",", "")

    # Convert "42.0" to "42" when it's a pure number
    try:
        if "." in ans:
            num = float(ans)
            if num == int(num):
                ans = str(int(num))
    except ValueError:
        pass

    return ans


def extract_answer(text: str) -> str:
    """
    Extract the final numeric answer from the model's output.
    - Prefer the number after '####' if present.
    - Otherwise, take the last integer/decimal in the text.
    """
    marker = "####"
    if marker in text:
        tail = text.split(marker)[-1]
        m = re.search(r"-?\d+\.?\d*", tail)
        if m:
            return m.group(0).strip()

    nums = re.findall(r"-?\d+\.?\d*", text)
    if nums:
        return nums[-1].strip()

    return text.strip()


def build_prompt(question: str) -> str:
    return (
        "You are a careful math tutor. Solve the problem step-by-step, "
        "then give the final answer in the format '#### 42'.\n\n"
        f"Problem:\n{question}\n\nSolution:\n"
    )


def eval_rl_on_platinum():
    """
    Evaluate DPO RL model on GSM8K Platinum test set with:
      - per-example JSONL logging
      - resume support (skip examples already logged)
    """
    print("=" * 80)
    print("GSM8K PLATINUM Evaluation: DPO RL Model (with resume + logging)")
    print("=" * 80)

    # ------------------------------------------------------------------
    # 1) Load dataset
    # ------------------------------------------------------------------
    print("\nLoading GSM8K Platinum test set from Hugging Face...")
    platinum_test = load_dataset("madrylab/gsm8k-platinum", "main", split="test")
    total_examples = len(platinum_test)
    print(f"Loaded {total_examples} test examples from GSM8K Platinum\n")

    # ------------------------------------------------------------------
    # 2) Prepare output directory and check for existing results
    # ------------------------------------------------------------------
    os.makedirs(OUT_DIR, exist_ok=True)

    start_index = 0
    correct_so_far = 0

    if os.path.exists(OUT_PATH):
        print(f"Found existing eval file at {OUT_PATH}, loading to resume...")
        with open(OUT_PATH, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                rec = json.loads(line)
                start_index += 1
                if rec.get("correct"):
                    correct_so_far += 1

        print(
            f"Already evaluated {start_index} examples "
            f"({correct_so_far} correct, acc={correct_so_far / max(1, start_index):.3f})"
        )
    else:
        print(f"No existing eval file found. Will write new results to {OUT_PATH}")

    # If everything was already done, exit early
    if start_index >= total_examples:
        final_acc = correct_so_far / total_examples
        print("\nAll examples already evaluated.")
        print("=" * 80)
        print(
            f"DPO RL Model on GSM8K Platinum: {final_acc:.3f} "
            f"({correct_so_far}/{total_examples})"
        )
        print("=" * 80)
        return final_acc

    # Open file in append mode for new results
    out_f = open(OUT_PATH, "a", encoding="utf-8")

    # ------------------------------------------------------------------
    # 3) Load model + tokenizer
    # ------------------------------------------------------------------
    base_model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
    lora_path = "checkpoints/lora_rl"

    print(f"\nLoading DPO RL model from {lora_path}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    # Optional safety: ensure pad_token exists
    if tokenizer.pad_token is None and tokenizer.eos_token is not None:
        tokenizer.pad_token = tokenizer.eos_token

    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        device_map="auto",
        load_in_8bit=True,
    )
    model = PeftModel.from_pretrained(base_model, lora_path)
    model.eval()

    # ------------------------------------------------------------------
    # 4) Main evaluation loop (with resume)
    # ------------------------------------------------------------------
    correct = correct_so_far

    print(
        f"\nStarting / resuming evaluation at index {start_index} "
        f"out of {total_examples} total examples.\n"
    )

    for i, ex in enumerate(platinum_test):
        if i < start_index:
            continue  # skip examples we've already logged

        q = ex["question"]
        gold = ex["answer"]

        prompt = build_prompt(q)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        input_length = inputs["input_ids"].shape[1]

        gen = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.6,
            top_p=0.9,
        )
        response_ids = gen[0][input_length:]
        text = tokenizer.decode(response_ids, skip_special_tokens=True)

        # Extract and normalize both answers
        pred_raw = extract_answer(text)
        gold_raw = extract_answer(gold)

        pred_norm = normalize_answer(pred_raw)
        gold_norm = normalize_answer(gold_raw)

        is_correct = (pred_norm == gold_norm) and (pred_norm != "")
        if is_correct:
            correct += 1

        # Write per-example record to JSONL
        record = {
            "index": i,
            "question": q,
            "gold_answer_raw": gold,
            "gold_answer_extracted": gold_raw,
            "gold_answer_normalized": gold_norm,
            "model_output": text,
            "pred_answer_extracted": pred_raw,
            "pred_answer_normalized": pred_norm,
            "correct": bool(is_correct),
        }
        out_f.write(json.dumps(record) + "\n")

        # Flush occasionally so progress is not lost if the job dies
        if (i + 1) % 10 == 0:
            out_f.flush()

        # Progress logging
        if (i + 1) % 50 == 0 or (i + 1) == total_examples:
            current_n = i + 1
            current_acc = correct / current_n
            print(
                f"  Progress: {current_n}/{total_examples}, "
                f"Current accuracy (over seen examples): {current_acc:.3f}"
            )

    out_f.close()

    # ------------------------------------------------------------------
    # 5) Final summary
    # ------------------------------------------------------------------
    acc = correct / total_examples
    print("\n" + "=" * 80)
    print(f"DPO RL Model on GSM8K Platinum: {acc:.3f} ({correct}/{total_examples})")
    print("=" * 80)
    print("Evaluation complete!")

    return acc


if __name__ == "__main__":
    eval_rl_on_platinum()
```

---

## 3. How this behaves in practice

* First run:

  * No `outputs/rl_platinum_eval.jsonl` → it starts from index 0.
  * Logs every example line by line.
  * If it dies at, say, 427, you’ll have 427 lines in the JSONL.

* Second run:

  * Finds existing JSONL.

  * Counts those 427 examples, and how many were correct.

  * Prints something like:

    > Already evaluated 427 examples (212 correct, acc=0.497)

  * Resumes at `i = 427` and continues to the end.

  * Final accuracy at the end is over the full test set.

* Later analysis:

  * You can load `outputs/rl_platinum_eval.jsonl` in a notebook and:

    * Filter by `correct == false`
    * Cluster by error type
    * Compare normalized vs extracted vs raw
    * Look at question difficulty, etc.

---

## 4. If you also want Base + SFT in this style

We can mirror this pattern to:

* a `eval_base_on_platinum()` that writes `outputs/base_platinum_eval.jsonl`
* a `eval_sft_on_platinum()` that writes `outputs/sft_platinum_eval.jsonl`

But since you’re currently focused on: *“did our RL actually do anything on Platinum?”*, starting with this RL-only script is totally fine.

If you want, next step I can give you a combined script:

* Base / SFT / RL
* Each with its own JSONL log
* All resumable the same way.
