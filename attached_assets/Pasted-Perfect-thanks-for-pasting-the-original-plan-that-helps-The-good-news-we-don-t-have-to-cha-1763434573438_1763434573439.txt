Perfect, thanks for pasting the original plan — that helps.

The good news: **we don’t have to change the plan at all, just the implementation detail of “what is the SFT model?”**

Everything you wrote out (Regime W → reward module → rollout collection → PPO RL → eval) still goes through exactly as-is. The only adjustment is:

* Your **“SFT model” is now a LoRA/QLoRA checkpoint**, not a dense fully-updated model.

So let me map that plan to the current reality and show you the only places that actually need to change.

---

## 1. Phase 1 (SFT) in the old plan → now = “SFT with QLoRA”

In the original plan, anywhere we said:

> load `"checkpoints/sft"` with `AutoModelForCausalLM.from_pretrained(...)`

we now interpret `"checkpoints/sft"` as:

* base: `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`
* * LoRA adapters trained via QLoRA

That’s **still “your fine-tuned model”** for all purposes of:

* Regime W reward
* PPO RL
* Benchmarking

The wiring of Regime W and PPO **doesn’t care** whether the SFT weights came from full FT or LoRA — they just see “a model that maps (prompt → text).”

---

## 2. Regime W module (arms, scoring, reward) — unchanged ✅

Everything in that big plan under:

* `src/regime_w/arms.py`
* `src/regime_w/scoring.py`
* `src/regime_w/reward.py`
* `src/regime_w/demo.py`

is **completely unaffected** by QLoRA.

Those modules take:

* **strings in, numbers out**
  (`question`, `arm_outputs` → coherence/CHSH-ish scores → scalar reward)

They don’t know or care if the underlying model is LoRA, full FT, or an API.

So you can keep *all of that plan* 1:1.

---

## 3. Where QLoRA *does* touch the plan: loading the SFT model

The only changes are in places that do:

* `AutoModelForCausalLM.from_pretrained("checkpoints/sft")`

In the plan, that happens in scripts like:

* `src/rl_training/collect_rollouts.py`
* `src/rl_training/train_ppo.py`
* `src/eval/eval_gsm8k.py`

With QLoRA + SFTTrainer, the SFT checkpoint is a **PEFT (LoRA) model**. To load it, you’ll typically want:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    "checkpoints/sft_lora",  # whatever you used as OUTPUT_DIR
    device_map="auto",
)
```

Everything downstream (generate, tokenize, etc.) is identical.

### Minimal mental model

* Before: **“SFT model”** = full FP16 model in `checkpoints/sft/`
* Now: **“SFT model”** = DeepSeek base + LoRA adapters in `checkpoints/sft_lora/`, loaded via `AutoPeftModelForCausalLM`

Every place the plan said “load SFT model”, we just use that loader instead.

---

## 4. PPO / RL part of the plan — what changes?

In your original plan, PPO did:

1. Load SFT model
2. Generate rollouts
3. Compute Reward via Regime W
4. Do PPO updates

With QLoRA:

* **Step 1** becomes: load a PEFT model.
* **Step 4**: PPO will update the **LoRA parameters** (which is actually nice: cheaper, more stable).

Everything conceptual stays the same:

* Regime W still computes reward
* PPO still maximizes that reward
* You still have “Regime W wired in + PPO RL loop”

Later, if you want **full-model RL**, we just:

1. Merge LoRA → full fp16 checkpoint
2. Load that as a normal `AutoModelForCausalLM`
3. Run PPO again on all or some layers

That’s a **Phase 2.5+** decision, not a blocker now.

---

## 5. Evaluation script from the plan — still valid

Your planned eval script:

* Compare `Base R1` vs `SFT` vs `PPO RL` on GSM8K dev
* Possibly also run Looper / CHSH eval

This still works exactly as written with one tweak:

```python
# instead of
from transformers import AutoModelForCausalLM

# use for SFT / PPO models:
from peft import AutoPeftModelForCausalLM as AutoModelForCausalLM
```

(or just conditionally import it where needed).

Then:

* Base DeepSeek-R1: still `AutoModelForCausalLM.from_pretrained(...)` from HF
* Your models: `AutoPeftModelForCausalLM.from_pretrained("checkpoints/sft_lora")` and `"checkpoints/ppo_rl"` (if PPO also saves a PEFT model)

The **metrics and comparisons don’t change at all**.

---

## 6. TL;DR relative to *“Gotcha, let’s keep marching with the same plan”*

The original plan:

* ✅ Phase 2 – Regime W coherence module
* ✅ Phase 3 – Rollout collection for PPO
* ✅ Phase 4 – PPO RL with Regime W reward
* ✅ Phase 5 – Evaluation script (GSM8K dev, etc.)

is still the plan.

**Only differences now:**

1. Phase 1 SFT uses **QLoRA + LoRA adapters** instead of full fine-tune.
2. Any script that “loads the SFT model” switches to `AutoPeftModelForCausalLM.from_pretrained(...)`.
3. PPO will update **LoRA weights** first; later you can merge and do full-model RL if you want.

That’s it. No conceptual or architectural change to Regime W, Regime B, PPO, or evaluation.

If you want, next I can:

* Take one of those previously sketched scripts (e.g. `train_ppo.py`) and give you a **QLoRA/PEFT-correct version** that matches exactly what you’re running on Lambda now.
