Multi-Path Coherence Signal (Regime W)

To move beyond pure correctness supervision, we introduce a proprietary multi-path coherence signal, which we refer to as Regime W.

For each math question, we generate multiple independent candidate solutions (trajectories) from the model, varying prompt phrasing and sampling parameters. Regime W analyzes this set of trajectories and produces:

agreement scores over final answers,

similarity scores over intermediate reasoning paths,

and internal consistency / counterfactual checks across trajectories.

These components are combined into a single scalar coherence score 
ùë†
wm
‚àà
[
0
,
1
]
s
wm
	‚Äã

‚àà[0,1], which measures how stable and self-consistent the model‚Äôs reasoning is for that question. Regime W is model-agnostic and used only during training; its implementation details and exact aggregation scheme are kept private.

Preference Construction from Coherence-Aware Rewards

For each training question, we:

Sample multiple candidate solutions from the SFT policy.

Evaluate each candidate using:

Outcome correctness (matches the gold numeric answer or not).

Regime W coherence score for the set of candidates.

A small length penalty to discourage unnecessarily long chains.

Convert these signals into a scalar reward per trajectory.

Construct preference pairs by selecting, for each question, a ‚Äúbetter‚Äù and ‚Äúworse‚Äù trajectory based on reward.

This yields a dataset of (prompt, better solution, worse solution) triplets that integrates both correctness and coherence information.

RL Fine-Tuning with QLoRA Adapters

We then perform a preference-based RL fine-tuning stage (DPO/GRPO-style) on top of the LoRA-SFT policy:

The policy model is the base model plus the SFT LoRA adapter.

A reference model is defined as a frozen copy of the SFT policy.

We optimize a preference-style objective that encourages the policy to assign higher probability to ‚Äúbetter‚Äù trajectories than to ‚Äúworse‚Äù ones, relative to the reference model.

Critically, only the LoRA adapters are updated in this stage; the base model weights remain frozen. This produces a new LoRA-RL adapter that encodes a preference for reasoning that is both correct and coherent under Regime W.

Inference and Evaluation

At inference time, no Regime W calls are made. Looper-Math-Platinum-8B is used as a standard single-call model:

question ‚Üí model ‚Üí chain-of-thought + final answer