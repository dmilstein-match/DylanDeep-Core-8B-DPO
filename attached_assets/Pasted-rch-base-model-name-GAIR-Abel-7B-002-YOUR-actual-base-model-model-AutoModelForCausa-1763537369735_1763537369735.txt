rch

base_model_name = 'GAIR/Abel-7B-002'  # ← YOUR actual base model

model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map='cpu'   # merge on CPU to avoid bugs
)
model = PeftModel.from_pretrained(model, 'checkpoints/abel_sft_lora')
model = model.merge_and_unload(safe_merge=True)   # ← fixes head_dim=None

tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)

model.save_pretrained('checkpoints/abel_sft_merged_fixed')
tokenizer.save_pretrained('checkpoints/abel_sft_merged_fixed')
print('✓ Fixed merge complete!')
"
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                  | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "<string>", line 8, in <module>
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5468, in _load_pretrained_model
    _error_msgs, disk_offload_index = load_shard_file(args)
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 831, in load_shard_file
    state_dict = load_state_dict(
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 508, in load_state_dict
    check_torch_load_is_safe()
  File "/home/ubuntu/looper-math-platinum-8b/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1647, in check_torch_load_is_safe
    raise ValueError(
ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.
See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434
(.venv) ubuntu@192-222-54-90:~/looper-math-platinum-8b$