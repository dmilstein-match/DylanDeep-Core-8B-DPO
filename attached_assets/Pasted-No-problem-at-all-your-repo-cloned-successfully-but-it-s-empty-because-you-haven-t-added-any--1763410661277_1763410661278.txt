No problem at all â€” **your repo cloned successfully, but itâ€™s empty** because you havenâ€™t added any files yet.
This is **normal** for a new repo.

So right now your structure is:

```
~/projects/
    looper-math-platinum-8b/
        (EMPTY)
```

This is exactly where we now add your project files.

---

# âœ… **Iâ€™ll give you the complete starter repo to paste into Replit**

Youâ€™ll copy/paste these folders into your **Replit project** (which is synced to GitHub).
Then youâ€™ll commit â†’ push from Replit.
Then on Lambda youâ€™ll just do `git pull`.

This way everything stays clean and synced.

---

# âœ… STEP 1 â€” In Replit, open your repo

Your Replit should already be linked to:

**dmilstein-match / looper-math-platinum-8b**

If not, create a new Repl â†’ **Import from GitHub** â†’ choose the repo.

---

# âœ… STEP 2 â€” Create these folders in Replit

Inside Replit, create this exact structure:

```
looper-math-platinum-8b/
    requirements.txt
    README.md
    src/
        baseline_sft/
            __init__.py
            prepare_data.py
            train_sft.py
        regime_w/
            __init__.py
            arms.py
            scoring.py
            reward.py
            demo.py
        rl_training/
            __init__.py
            collect_rollouts.py
            train_ppo.py
        eval/
            __init__.py
            eval_platinum.py
```

Iâ€™ll give you **the exact contents** for each of these files below.

---

# ðŸ“Œ **Create `requirements.txt`**

Paste this:

```txt
transformers
datasets
accelerate
peft
trl
bitsandbytes
sentencepiece
numpy
scipy
```

---

# ðŸ“Œ **Create `README.md`**

Simple starter:

```md
# Looper-Math-Platinum-8B

Private project for GSM8K-Platinum fine-tuning with:
- baseline SFT
- Regime W coherence module
- PPO RL optimization

Training runs on Lambda A100 40GB.
Code edited in Replit, synced with GitHub.
```

---

# ðŸ“Œ **Now create the code files (Iâ€™ll give minimal plugs)**

### `src/baseline_sft/prepare_data.py`

```python
from datasets import load_dataset
import json
import os

def main():
    os.makedirs("data", exist_ok=True)

    ds = load_dataset("gsm8k", "main")

    train = ds["train"]
    test = ds["test"]

    with open("data/gsm8k_train.jsonl", "w") as f:
        for x in train:
            f.write(json.dumps(x) + "\n")

    with open("data/gsm8k_test.jsonl", "w") as f:
        for x in test:
            f.write(json.dumps(x) + "\n")

    print("Saved GSM8K train/test splits.")

if __name__ == "__main__":
    main()
```

---

### `src/baseline_sft/train_sft.py`

```python
import os
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTTrainer

BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"

def load_data(path):
    data = []
    with open(path, "r") as f:
        for line in f:
            j = json.loads(line)
            prompt = f"You are a careful math tutor. Solve step by step.\n\nProblem:\n{j['question']}\n\nSolution:"
            target = f"{j['answer']}"
            data.append({"prompt": prompt, "completion": target})
    return data

def main():
    print("Loading data...")
    data = load_data("data/gsm8k_train.jsonl")

    print("Loading model...")
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map="auto")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=data,
        dataset_text_field="prompt",
        max_seq_length=2048,
        output_dir="checkpoints/sft"
    )

    trainer.train()
    trainer.save_model()

if __name__ == "__main__":
    main()
```

---

# ðŸ“Œ `src/regime_w/arms.py`

```python
def build_all_arms():
    arms = []
    temps = [0.2, 0.4, 0.6, 0.8]
    for i, t in enumerate(temps):
        arms.append({
            "name": f"wolfram_{i}",
            "system": "You are a formal reasoning assistant.",
            "temp": t,
            "top_p": 0.95
        })
    for i, t in enumerate(temps):
        arms.append({
            "name": f"maudlin_{i}",
            "system": "Explain your reasoning and reflect on counterfactual steps.",
            "temp": t,
            "top_p": 0.9
        })
    return arms
```

---

# ðŸ“Œ `src/regime_w/scoring.py`

```python
import numpy as np

def s_end_for_question(answers):
    # simple pairwise agreement
    n = len(answers)
    agree = 0
    total = 0
    for i in range(n):
        for j in range(i+1, n):
            total += 1
            if answers[i] == answers[j]:
                agree += 1
    return agree / total if total > 0 else 0

def s_path_for_question(reasonings):
    # dummy placeholder
    return 0.5

def s_cf_for_question(outputs):
    # dummy placeholder
    return 0.5

def s_wm_for_question(s_end, s_path, s_cf):
    return 0.4*s_end + 0.3*s_path + 0.3*s_cf
```

---

# ðŸ“Œ `src/regime_w/reward.py`

```python
from .scoring import (
    s_end_for_question,
    s_path_for_question,
    s_cf_for_question,
    s_wm_for_question
)

def compute_reward(question, arm_outputs, gold=None):
    answers = [x["answer"] for x in arm_outputs]
    reasonings = [x["reasoning"] for x in arm_outputs]

    s_end = s_end_for_question(answers)
    s_path = s_path_for_question(reasonings)
    s_cf = s_cf_for_question(arm_outputs)

    s_wm = s_wm_for_question(s_end, s_path, s_cf)

    correct = any(a == gold for a in answers)

    base = 1.0 if correct else -0.5
    bonus = 0.7 * s_wm

    return base + bonus
```

---

# ðŸ“Œ `src/rl_training/train_ppo.py`

```python
# skeleton file
from trl import PPOTrainer, PPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from src.regime_w.reward import compute_reward

def main():
    print("PPO training goes here...")
    # You will flesh this out after SFT works.

if __name__ == "__main__":
    main()
```

---

# ðŸ“Œ `src/eval/eval_platinum.py`

```python
print("Eval script placeholder. We'll fill this once PPO training is ready.")
```

---

# ðŸ“Œ **STEP 3 â€” Commit & Push from Replit**

In Replit:

```
git add .
git commit -m "initial project scaffold"
git push
```

Now your repo on GitHub contains the full structure.

---

# ðŸ“Œ **STEP 4 â€” Pull it to your Lambda box**

Back in your SSH terminal (Lambda):

```bash
cd ~/projects/looper-math-platinum-8b
git pull
```

Now you will see all the files there.

---

# ðŸŽ‰ At this point:

Youâ€™re fully connected:

* Lambda GPU is ready
* Repo is populated
* Replit is your editor
* Training scripts can now run

Next weâ€™ll run:

### `python -m src.baseline_sft.prepare_data`

Then:

### `python -m src.baseline_sft.train_sft`

If you want, I can walk you through exactly how to run your **first SFT training run** right now.
