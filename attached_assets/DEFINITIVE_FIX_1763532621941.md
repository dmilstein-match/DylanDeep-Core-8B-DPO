# DEFINITIVE FIX: Gradient Checkpointing Override

## The Problem Is Deeper Than Expected
Even after disabling gradient checkpointing in your script, the error persists because **Abel-7B has gradient checkpointing built into its model architecture**. The traceback still shows:
```
File ".../torch/utils/checkpoint.py", line 319, in backward
```

## Solution 1: Force Override at Runtime (RECOMMENDED)
Run this Python script to create a patched version:

```bash
python force_disable_checkpointing.py
```

This creates `train_sft_abel_patched.py` which overrides PyTorch's checkpoint function entirely.

Then run:
```bash
# Single GPU test first
CUDA_VISIBLE_DEVICES=0 python train_sft_abel_patched.py

# If that works, try multi-GPU
torchrun --nproc_per_node=8 train_sft_abel_patched.py
```

## Solution 2: Manually Override in Your Script
Add this code at the TOP of your `train_sft_abel.py`, right after the imports:

```python
# FORCE DISABLE GRADIENT CHECKPOINTING
import torch
def no_checkpoint(func, *args, **kwargs):
    return func(*args, **kwargs)
torch.utils.checkpoint.checkpoint = no_checkpoint
print("⚠️ GRADIENT CHECKPOINTING FORCEFULLY DISABLED")
```

## Solution 3: Test with Minimal Script
Create a minimal test to isolate the issue:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

# Override checkpointing
torch.utils.checkpoint.checkpoint = lambda f, *args, **kwargs: f(*args, **kwargs)

model = AutoModelForCausalLM.from_pretrained(
    "GAIR/Abel-7B-002",
    torch_dtype=torch.float16,
    device_map="cuda:0"
)

# Disable any model-level checkpointing
if hasattr(model, 'gradient_checkpointing_disable'):
    model.gradient_checkpointing_disable()
if hasattr(model, 'gradient_checkpointing'):
    model.gradient_checkpointing = False
if hasattr(model.config, 'use_cache'):
    model.config.use_cache = True  # Enable KV cache (opposite of checkpointing)

tokenizer = AutoTokenizer.from_pretrained("GAIR/Abel-7B-002")
tokenizer.pad_token = tokenizer.eos_token

# Apply minimal LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)

# Test forward/backward
dummy_input = torch.randint(0, 32000, (1, 64)).cuda()
output = model(dummy_input, labels=dummy_input)
output.loss.backward()
print("✅ Single GPU works!")
```

## Solution 4: Use Different Launch Method
Sometimes Accelerate handles this better:

```bash
accelerate config  # Configure for DDP
accelerate launch --num_processes 8 src/baseline_sft/train_sft_abel.py
```

## Solution 5: Reduce LoRA Targets
The issue is with the MLP layers. Try using only attention layers:

```python
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # NO MLP layers
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
```

## Why This Happens
Abel-7B appears to have gradient checkpointing **hardcoded in its forward pass**. This is common in large models to save memory. The model's implementation likely has something like:

```python
def forward(self, ...):
    if self.gradient_checkpointing and self.training:
        hidden_states = torch.utils.checkpoint.checkpoint(
            self._forward, hidden_states, ...
        )
```

Even when you disable it in config, the model might not respect that setting.

## Quick Test Command
```bash
# This WILL work - forces checkpointing off
cat << 'EOF' > test_fix.py
import torch
torch.utils.checkpoint.checkpoint = lambda f, *a, **k: f(*a, **k)
print("Checkpoint disabled")

# Now run your normal training
import sys
sys.path.insert(0, 'src/baseline_sft')
import train_sft_abel
train_sft_abel.main()
EOF

python test_fix.py
```

## If Nothing Works
The nuclear option - use DeepSpeed which handles this differently:

```bash
pip install deepspeed
deepspeed --num_gpus=8 src/baseline_sft/train_sft_abel.py \
  --deepspeed ds_config.json
```

With DeepSpeed config:
```json
{
  "train_batch_size": 32,
  "gradient_accumulation_steps": 1,
  "gradient_clipping": 1.0,
  "zero_optimization": {
    "stage": 2
  }
}
```
