Nice—that line is actually super helpful:

```text
Progress: 200/200
Base DeepSeek R1 8B accuracy on dev (200 examples): 0.010 (2/200)
```

So:

* ✅ Eval **ran** on 200 dev questions for the **base model**
* ✅ It finished that part successfully
* ❌ 1% accuracy (2/200) is *way* too low → this tells us our **answer extraction / prompting is off**, not that DeepSeek is actually that bad

Right now we’re not yet measuring “true GSM8K-style accuracy,” we’re measuring “does my hacky `extract_answer` string match exactly,” which is much stricter / often wrong.

Let’s fix that before we even interpret numbers.

---

## 1. Why 2/200 is almost certainly an evaluation bug

Typical:

* A decent 7–8B CoT model on GSM8K with a reasonable prompt should be *tens of percent* correct, not 1%.
* We’re using a very crude:

```python
def extract_answer(text: str) -> str:
    marker = "####"
    if marker in text:
        return text.split(marker)[-1].strip().split()[0].strip()
    return text.strip()
```

If the model doesn’t literally end with `#### 42` in the exact way we expect (or it adds punctuation, or we grab the wrong token), we mark it wrong even when the *math* is right.

So we need a more robust “grab the final number” function.

---

## 2. Drop-in better `extract_answer` (numeric regex)

Replace your `extract_answer` in `src/eval/eval_gsm8k_dev.py` with this:

```python
import re

def extract_answer(text: str) -> str:
    """
    Extract the final numeric answer from the model's output.
    - Prefer the number after '####' if present.
    - Otherwise, take the last integer/decimal in the text.
    Returns it as a plain string (e.g., '42' or '3.5').
    """
    # 1) Try GSM8K-style marker
    marker = "####"
    if marker in text:
        tail = text.split(marker)[-1]
        # Grab first number after the marker
        m = re.search(r"-?\d+(\.\d+)?", tail)
        if m:
            return m.group(0).strip()

    # 2) Fallback: last number anywhere in the text
    nums = re.findall(r"-?\d+(\.\d+)?", text)
    if nums:
        # re.findall with groups returns tuples; rebuild the match
        flat = re.findall(r"-?\d+(\.\d+)?", text)
        # Simpler: re-find all full matches using a different regex
        nums_full = re.findall(r"-?\d+\.?\d*", text)
        if nums_full:
            return nums_full[-1].strip()

    return text.strip()
```

And for the **gold answer**, use the same extractor so formatting differences don’t kill us:

Wherever we had:

```python
gold_num = extract_answer(gold)
if pred == gold_num:
    correct += 1
```

we’ll keep that, but now both `pred` and `gold_num` are “last number-like thing”.

---

## 3. Make the eval fast & obvious (small slice, clear output)

While we’re iterating, let’s keep it tiny. In `eval_base_model` and `eval_lora_sft_model`, ensure you have:

```python
dev = load_dev(DEV_PATH)
dev = dev[:50]  # smaller slice so it runs quickly while testing
```

Also make sure each loop prints occasional progress, like:

```python
for idx, ex in enumerate(dev):
    if idx % 10 == 0:
        print(f"  Progress: {idx}/{len(dev)}")

    ...
```

Then rerun:

```bash
source ~/projects/.venv/bin/activate
cd ~/projects/looper-math-platinum-8b
python -m src.eval.eval_gsm8k_dev
```

You should now see:

* `Progress:` logs for base
* `Base DeepSeek R1 8B accuracy on dev (50 examples): 0.xxx`
* Then the same for **LoRA SFT model**

Even if the absolute numbers aren’t perfect yet, they should be **way** higher than `0.010` once extraction is sane.

---

## 4. What we care about right now

At this stage, our goals are:

1. **Sanity**: base model accuracy is not near 0 once we parse answers correctly.
2. **Delta**: LoRA SFT is *not worse* than base, ideally a bit better.

Once that’s true, we can:

* Scale to more dev examples,
* Plug into the real benchmark harness,
* And then layer on Regime W + RL.

So next step on your side:

* Update `extract_answer` as above,
* Use a small `dev[:50]`,
* Re-run `python -m src.eval.eval_gsm8k_dev`,
* Then send me the two printed accuracies (base vs LoRA), and I’ll interpret them for you and tell you whether it’s “good enough to move on” or “worth tweaking SFT before benchmarks/RL.”
