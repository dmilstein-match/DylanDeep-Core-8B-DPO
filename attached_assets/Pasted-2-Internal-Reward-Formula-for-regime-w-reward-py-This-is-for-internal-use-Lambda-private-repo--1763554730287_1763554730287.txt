2️⃣ Internal Reward Formula for regime_w/reward.py
This is for internal use (Lambda / private repo) so your dev doesn’t have to guess.
We’ll assume:


There’s a function compute_regime_w_metrics(...) in scoring.py that returns question-level coherence metrics: s_end, s_path, s_cf, s_wm.


You can compute correctness per trajectory.


You know how to count token length (from tokenizer).


Here’s a concrete design that’s:


simple,


stable,


and directly aligned with what you’ve been doing conceptually.


2.1 Data structures
In regime_w/reward.py:
from dataclasses import dataclass
from typing import List, Dict, Any

from .scoring import compute_regime_w_metrics  # you or dev implement this elsewhere


@dataclass
class Trajectory:
    text: str           # full generated text (CoT + final answer)
    reasoning: str      # optional; can be same as text
    answer: str         # parsed final numeric answer, e.g. "42"
    num_tokens: int     # length of text in tokens

2.2 Reward hyperparameters
Put these at the top so you can tweak later:
# correctness weight
ALPHA_CORRECT = 1.0

# coherence weight (question-level s_wm)
BETA_COHERENCE = 0.5

# length penalty (per token beyond a threshold)
LENGTH_PENALTY_PER_TOKEN = 0.001
LENGTH_PENALTY_THRESHOLD = 512  # tokens

# small bonus for questions where all correct trajectories agree
FULL_AGREEMENT_BONUS = 0.2

2.3 Helper: correctness comparison
def normalize_answer(ans: str) -> str:
    # Very simple normalization; dev can make this match GSM8K utils.
    return ans.strip().lower()

2.4 Main reward function
This is the part your dev actually wires into the RL pipeline:
def compute_rewards_for_question(
    question: str,
    trajectories: List[Trajectory],
    gold_answer: str,
) -> List[float]:
    """
    Given one question, a list of trajectories from the policy,
    and the gold answer, return a reward per trajectory.

    This is the core Regime W -> scalar reward mapping.
    """

    # 1) basic correctness flags per trajectory
    gold = normalize_answer(gold_answer)
    correct_flags = []
    for t in trajectories:
        pred = normalize_answer(t.answer)
        correct_flags.append(1.0 if pred == gold and gold != "" else 0.0)

    # 2) compute question-level coherence metrics (IP-sensitive, implemented elsewhere)
    #    You pass *all* trajectories so scoring can compute agreement, path similarity, etc.
    metrics = compute_regime_w_metrics(question, trajectories)
    # expected keys: "s_end", "s_path", "s_cf", "s_wm"
    s_end = metrics.get("s_end", 0.0)
    s_path = metrics.get("s_path", 0.0)
    s_cf = metrics.get("s_cf", 0.0)
    s_wm = metrics.get("s_wm", 0.0)

    # 3) optional bonus if *all* correct trajectories agree nicely
    any_correct = any(c > 0.5 for c in correct_flags)
    all_correct_same_answer = False
    if any_correct:
        correct_answers = {
            normalize_answer(t.answer)
            for t, c in zip(trajectories, correct_flags)
            if c > 0.5
        }
        all_correct_same_answer = len(correct_answers) == 1

    # 4) compute per-trajectory rewards
    rewards: List[float] = []
    for t, correct in zip(trajectories, correct_flags):
        # base term: correctness
        base = ALPHA_CORRECT * correct   # 1.0 if correct, 0.0 if not

        # coherence term: same for all trajectories on this question
        coherence_term = BETA_COHERENCE * s_wm

        # length penalty per trajectory
        extra_tokens = max(0, t.num_tokens - LENGTH_PENALTY_THRESHOLD)
        length_penalty = LENGTH_PENALTY_PER_TOKEN * float(extra_tokens)

        # optional full-agreement bonus applied only to correct trajectories
        agreement_bonus = 0.0
        if correct > 0.5 and all_correct_same_answer and s_end > 0.9:
            agreement_bonus = FULL_AGREEMENT_BONUS

        reward = base + coherence_term + agreement_bonus - length_penalty
        rewards.append(reward)

    return rewards

2.5 Public vs Internal


Internal (private repo / Lambda): you can use this as-is. It’s specific enough for your dev to implement and debug.


Public (if you ever open-source parts of the training loop):
You swap this file for something like:
def compute_rewards_for_question(question, trajectories, gold_answer):
    """
    Placeholder: calls a proprietary coherence scoring service.
    """
    raise NotImplementedError("This project uses a private coherence service.")



and never ship the metrics / arms / formulas.

3️⃣ How this plugs into RL code
Your RL pipeline (DPO/GRPO) will:


Generate trajectories (with LoRA-SFT policy).


Wrap them as Trajectory objects.


Call:
rewards = compute_rewards_for_question(question, trajectories, gold_answer)



Use rewards to:


either pick best/worst for preference pairs, or


use directly as scalar reward in PPO-style RL.




You don’t need to change any math in Regime W itself for LoRA vs full fine-tune — Regime W just sees strings.

If you want, next I can:


Write the exact DPOTrainer-style training loop that calls compute_rewards_for_question and builds pairs, or


Draft a short “Results” section you can use later once you have GSM8K-Platinum numbers.


